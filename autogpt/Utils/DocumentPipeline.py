import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TRANSFORMERS_OFFLINE"] = "1" 
from pathlib import Path
from typing import List, Dict, Any
from langchain_community.document_loaders import (
    UnstructuredPDFLoader,
    UnstructuredWordDocumentLoader,
    TextLoader,
)
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import pandas as pd
from markdownify import markdownify as md2text

class DocumentPipeline:
    """
    é€šç”¨æ–‡æ¡£å¤„ç†ç®¡é“ï¼šæ”¯æŒ PDF, Markdown, Excel, Word ,txt ç­‰æ ¼å¼
    è¾“å‡ºï¼šChroma å‘é‡æ•°æ®åº“
    """

    def __init__(
        self,
        data_dir: str,
        persist_dir: str = "./chroma_db",
        #embedding_model: str = "all-MiniLM-L6-v2",
        embedding_model: str = "/Users/pc/Documents/models/sentence-transformers/all-MiniLM-L6-v2",
        chunk_size: int = 512,
        chunk_overlap: int = 50,
    ):
        self.data_dir = Path(data_dir)
        if not self.data_dir.exists():
            raise ValueError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")

        self.persist_dir = persist_dir
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedding = HuggingFaceEmbeddings(
            model_name = embedding_model,
            model_kwargs={"device": "cpu"} 
        )

        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=300,          # ä¸­æ–‡å»ºè®® 200~500 å­—ç¬¦
            chunk_overlap=50,        # ä¿ç•™ä¸Šä¸‹æ–‡è¡”æ¥
            length_function=len,
            is_separator_regex=False,
        )

        # å‘é‡æ•°æ®åº“
        self.vectorstore = None

    def load_markdown_files(self) -> List[Dict[str, Any]]:
        """åŠ è½½ Markdown æ–‡ä»¶ â†’ è½¬ä¸ºçº¯æ–‡æœ¬"""
        docs = []
        for md_path in self.data_dir.rglob("*.md"):
            try:
                with open(md_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                    text = md2text(html_content)  # è½¬ä¸ºçº¯æ–‡æœ¬
                docs.append({
                    "page_content": text,
                    "metadata": {"source": str(md_path), "type": "markdown"}
                })
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ Markdown æ–‡ä»¶ {md_path}: {e}")
        return docs

    def load_excel_files(self) -> List[Dict[str, Any]]:
        """åŠ è½½ Excel æ–‡ä»¶ â†’ æ¯ä¸ª sheet è½¬ä¸ºè¡¨æ ¼æ–‡æœ¬"""
        docs = []
        for file_path in self.data_dir.rglob("*.xlsx"):
            try:
                excel_file = pd.ExcelFile(file_path)
                for sheet_name in excel_file.sheet_names:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    # è½¬ä¸º Markdown è¡¨æ ¼æ ¼å¼ï¼ˆå¯è¯»æ€§å¼ºï¼‰
                    table_text = df.to_markdown(index=False) if hasattr(df, 'to_markdown') else str(df)
                    docs.append({
                        "page_content": table_text,
                        "metadata": {
                            "source": str(file_path),
                            "sheet_name": sheet_name,
                            "type": "excel"
                        }
                    })
                excel_file.close()
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ Excel æ–‡ä»¶ {file_path}: {e}")
        return docs

    def load_csv_files(self) -> List[Dict[str, Any]]:
        """åŠ è½½ CSV æ–‡ä»¶"""
        docs = []
        for csv_path in self.data_dir.rglob("*.csv"):
            try:
                loader = CSVLoader(str(csv_path))
                raw_docs = loader.load()
                for doc in raw_docs:
                    doc.metadata["type"] = "csv"
                docs.extend(raw_docs)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ CSV æ–‡ä»¶ {csv_path}: {e}")
        return docs

    def load_pdf_files(self) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ pdfplumber æå– PDF æ–‡æœ¬"""
        from pdfplumber import open as pdf_open
        documents = []
        #pdf_dir = Path(pdf_path)
        for pdf_path in self.data_dir.glob("**/*.pdf"):
            print(f"pdf_path: {pdf_path}")
            text = ""
            with pdf_open(pdf_path) as pdf:
                for page in pdf.pages:
                    text += page.extract_text() or ""
            # æ¨¡æ‹Ÿ LangChain Document
            documents.append({
                "page_content": text,
                "metadata": {"source": str(pdf_path), "type": "pdf"}
            })
        return documents

    def load_pdf_files1(self) -> List[Dict[str, Any]]:
        """åŠ è½½ PDF æ–‡ä»¶"""
        docs = []
        for pdf_path in self.data_dir.rglob("*.pdf"):
            try:
                loader = UnstructuredPDFLoader(str(pdf_path))
                raw_docs = loader.load()
                for doc in raw_docs:
                    doc.metadata["type"] = "pdf"
                    doc.metadata["source"] = str(pdf_path)
                docs.extend(raw_docs)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ PDF æ–‡ä»¶ {pdf_path}: {e}")
        return docs

    def load_word_files(self) -> List[Dict[str, Any]]:
        """åŠ è½½ Word æ–‡ä»¶ (.docx)"""
        docs = []
        for docx_path in self.data_dir.rglob("*.docx"):
            try:
                loader = UnstructuredWordDocumentLoader(str(docx_path)) # ä¸‹è½½çš„åŒ…ä¸numpyå†²çª
                raw_docs = loader.load()
                for doc in raw_docs:
                    doc.metadata["type"] = "word"
                    doc.metadata["source"] = str(docx_path)
                docs.extend(raw_docs)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ Word æ–‡ä»¶ {docx_path}: {e}")
        return docs

    def load_text_files(self) -> List[Dict[str, Any]]:
        """åŠ è½½çº¯æ–‡æœ¬æ–‡ä»¶ (.txt)"""
        docs = []
        for txt_path in self.data_dir.rglob("*.txt"):
            try:
                loader = TextLoader(str(txt_path), encoding='utf-8')
                raw_docs = loader.load()
                for doc in raw_docs:
                    doc.metadata["type"] = "text"
                docs.extend(raw_docs)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡æœ¬æ–‡ä»¶ {txt_path}: {e}")
        return docs

    def load_all_documents(self) -> List[Dict[str, Any]]:
        """ç»Ÿä¸€åŠ è½½æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£ç±»å‹"""
        print("ğŸ” å¼€å§‹åŠ è½½æ–‡æ¡£...")
        all_docs = []

        # æŒ‰ç±»å‹åŠ è½½
        all_docs.extend(self.load_pdf_files())
        all_docs.extend(self.load_word_files())
        all_docs.extend(self.load_markdown_files())
        all_docs.extend(self.load_excel_files())
        all_docs.extend(self.load_csv_files())
        all_docs.extend(self.load_text_files())

        print(f"âœ… å…±åŠ è½½ {len(all_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        return all_docs

    def split_documents(self, raw_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆ†å—å¤„ç†"""
        print("âœ‚ï¸ æ­£åœ¨åˆ†å—...")
        split_docs = []
        for item in raw_docs:
            content = item["page_content"]
            metadata = item["metadata"]

            # åˆ†å—
            chunks = self.text_splitter.split_text(content)
            for chunk in chunks:
                split_docs.append({
                    "page_content": chunk,
                    "metadata": metadata
                })

        print(f"âœ… åˆ†å—å®Œæˆï¼Œå…± {len(split_docs)} ä¸ªæ–‡æœ¬å—")
        return split_docs

    def build_vectorstore(self, documents: List[Dict[str, Any]]):
        """æ„å»º Chroma å‘é‡æ•°æ®åº“"""
        print("ğŸ“¦ æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“...")

        texts = [doc["page_content"] for doc in documents]
        metadatas = [doc["metadata"] for doc in documents]
        # use langchain collection name as default !
        self.vectorstore = Chroma.from_texts(
            texts = texts,
            embedding = self.embedding,
            metadatas = metadatas,
            persist_directory = self.persist_dir
        )
        self.vectorstore.persist()
        print(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³: {self.persist_dir}")

    def run(self):
        """ä¸€é”®è¿è¡Œæ•´ä¸ªç®¡é“"""
        raw_docs = self.load_all_documents()
        if not raw_docs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
            return

        split_docs = self.split_documents(raw_docs)
        self.build_vectorstore(split_docs)
        print("ğŸ‰ æ–‡æ¡£å¤„ç†ç®¡é“æ‰§è¡Œå®Œæˆï¼")

    def as_retriever(self,k:int):
        """è¿”å› retrieverï¼Œç”¨äºåç»­é—®ç­”"""
        if not self.vectorstore:
            self.vectorstore = Chroma(
                persist_directory = self.persist_dir,
                embedding_function = self.embedding
            )
        return self.vectorstore.as_retriever(search_kwargs={"k": k})